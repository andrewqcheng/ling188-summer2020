{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP through text classification\n",
    "\n",
    "This notebook is an introduction to NLP through text classification. \n",
    "\n",
    "We'll cover the following topics:\n",
    "\n",
    "[What is text classification?](#what)\n",
    "\n",
    "_What is text classification and why should you care?_\n",
    "\n",
    "[Exploratory Data Analysis](#eda)\n",
    "\n",
    "_It's always a good idea to get to know your data before doing anything else._\n",
    "\n",
    "[Featurization](#featurization)\n",
    "\n",
    "_How can we turn natural language data into something we can do machine learning on?_\n",
    "\n",
    "[Classification](#classification)\n",
    "\n",
    "_How can a computer learn to distinguish between different categories?_\n",
    "\n",
    "[Interpretation](#interpret)\n",
    "\n",
    "_Did the computer learn correctly? Can the computer tell us anything new about our data?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is text classification?<a id='what'></a>\n",
    "\n",
    "Imagine that you work at [YouTube](https://www.youtube.com/) (if you haven't heard of it, YouTube is a video-sharing website). Your job is to remove comments on videos that are spam (unsolicited and inappropriate comments). You look through each video and read the comments yourself, deciding which are spam and which are not spam. Perhaps you see comments like those below. Which would you consider to be spam and which not spam?\n",
    "\n",
    "- _Hey @dancer317, love ur videos so much! Thanks for all the tips on dancing!_\n",
    "- _OUR  LASER PRINTER/FAX/COPIER TONER CARTRIDGE PRICES NOW AS LOW AS 39 DOLLARS. SPECIALS WEEKLY ON ALL LASER PRINTER SUPPLIES. WE CARRY MOST ALL LASER PRINTER CARTRIDGES, FAX SUPPLIES AND COPIER TONERS AT WAREHOUSE PRICES_\n",
    "- _I'm not sold on your first point about crossing national boundaries, but I see what you mean about non-economic alternatives._\n",
    "- _Some of the most beautiful women in the world bare it all for you. Denise Richards, Britney  Spears, Jessica Simpson, and many more. CLICK HERE FOR NUDE CELEBS_\n",
    "\n",
    "How did you decide which were spam and which weren't? Maybe one thing you noted was the high number of words in all capitals. The topics can also give you a clue, as the spam-like comments talk about selling things and nudity, which are often found in spam comments.\n",
    "\n",
    "However you decided, we can think about the task you were doing like this:\n",
    "\n",
    "<img src='img/human-classification.jpg' />\n",
    "\n",
    "This is text classification, performed by a human. What you just did was an example of text classification. You took a comment written in English, and you classified it into one of two classes: spam or not spam. Wouldn't it be nice to have a computer do this for you? [You could outsource your job to the computer and just surf the web all day](https://www.npr.org/sections/thetwo-way/2013/01/16/169528579/outsourced-employee-sends-own-job-to-china-surfs-web). What you'd want to do is replace the human with a computer, like this:\n",
    "\n",
    "\n",
    "<img src='img/computer-classification.jpg' />\n",
    "\n",
    "How are we going to do this? Well, what if, for each comment on YouTube, we counted the number of times it mentioned nudity or tried to sell something, and we measured the proportion of capital letters? We'd get two numbers for each comment. We could also use your human judgements from before in a third column telling us whether that comment is spam or not.\n",
    "\n",
    "| Comment                                                 | Selling or nudity | Proportion capital letters | Is it spam? |\n",
    "|---------------------------------------------------------|-------------------|----------------------------|-------------|\n",
    "| Hey @dancer317, love ur videos so much! Thanks for ...  | 0                 | 0.1                        | No          |\n",
    "| OUR LASER PRINTER/FAX/COPIER TONER CARTRIDGE PRICES ... | 4                 | 1.0                        | Yes         |\n",
    "| I'm not sold on your first point ...                    | 1                 | 0.05                       | No          |\n",
    "|  Some of the most beautiful women in the world ...      | 3                 | 0.15                       | Yes         |\n",
    "\n",
    "We can treat these two numbers as geometric coordinates and plot them. We can plot the spam comments in red and the non-spam comments in green.\n",
    "\n",
    "<img src='img/classification-no-line.jpg' />\n",
    "\n",
    "<img src='img/classification-with-line.jpg' />\n",
    "\n",
    "**To do text classification, we're going to need to do two things:**\n",
    "- **Turn our natural language comments into numbers.**\n",
    "- **Train a classifier to take those numbers and distinguish between the classes.**\n",
    "\n",
    "Why do we care about text classification? Because most applied natural language processing problems can be tackled as text classification:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Genre classification\n",
    "- Language identification\n",
    "- Authorship attribution\n",
    "- Is this document relevant to this legal case?\n",
    "- Is the patient in need of urgent care?\n",
    "\n",
    "### What is sentiment analysis?\n",
    "\n",
    "In this notebook, we're going to perform [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) on a dataset of tweets about US airlines. Sentiment analysis is the task of extracting [affective states][1] from text. Sentiment analysis is most ofen used to answer questions like:\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Affect_(psychology)\n",
    "\n",
    "- _what do our customers think of us?_\n",
    "- _do our users like the look of our product?_\n",
    "- _what aspects of our service are users dissatisfied with?_\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset was collected by [Crowdflower](https://www.crowdflower.com/), which they then made public through [Kaggle](https://www.kaggle.com/crowdflower/twitter-airline-sentiment). I've downloaded it for you and put it in the \"data\" directory. Note that this is a nice clean dataset; not the norm in real-life data science! I've chosen this dataset so that we can concentrate on understanding what text classification is and how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datascience import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "sns.set()\n",
    "sns.set_context('notebook')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis<a id='eda'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table.read_table(\"tweets.csv\")\n",
    "table.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which airlines are tweeted about and how many of each in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.group_bar(\"airline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "- How many tweets are in the dataset?\n",
    "- How many tweets are positive, neutral and negative?\n",
    "- Visualize the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "print(\"There are\", table.num_rows, \"tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "table.group(\"airline_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "table.group_bar(\"airline_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra challenge\n",
    "\n",
    "- When did the tweets come from?\n",
    "- Who gets more retweets: positive, negative or neutral tweets?\n",
    "- What are the three main reasons why people are tweeting negatively? What could airline companies do to improve this?\n",
    "- What's the distribution of time zones in which people are tweeting?\n",
    "- Is this distribution consistent depending on what airlines they're tweeting about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization<a id='featurization'></a>\n",
    "\n",
    "How are we going to turn our tweets into numbers? Well first, I want to do some quick preprocessing to remove some junk. To that that, we're going to use regular expressions. We won't cover regular expressions in detail here, but if you're interested in learning more, please see [here](https://github.com/geoffbacon/regular-expressions-in-python). For our purposes, a regular expression is a sequence of characters that defines a search pattern. We can use regular expressions to find particular patterns in text data. The text data could be an English sentence, or e-mail addresses, or TeX commands, Python source code, or anything you like. Once we've found a particular pattern, we can optionally replace it with some other text. **In this way, regular expressions are really just advanced \"find and replace\" techniques.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_handle_pattern = r'@(\\w+)'\n",
    "hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "url_pattern = r'https?:\\/\\/.*.com'\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(hashtag_pattern, ' HASHTAG', tweet)\n",
    "    tweet = re.sub(twitter_handle_pattern, 'USER', tweet)\n",
    "    tweet = re.sub(url_pattern, 'URL', tweet)\n",
    "    return tweet\n",
    "\n",
    "my_tweets = [\"lol @justinbeiber and @BillGates are like soo #yesterday #amiright saw it on https://twitter.com #yolo\",\n",
    "            'omg I am never flying on Delta again',\n",
    "            'I love @VirginAmerica so much #friendlystaff']\n",
    "\n",
    "for tweet in my_tweets:\n",
    "    print(clean_tweet(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = table.apply(clean_tweet, \"text\")\n",
    "table.append_column(\"clean_text\", clean_text)\n",
    "table.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "Now that we've cleaned the text, we need to turn the text into numbers for our classifier. We're going to use a \"bag of words\" as our features. A bag of words is just like a frequency count of all the words that appear in a tweet. It's called a bag because we ignore the order of the words; we just care about what words are in the tweet. To do this, we can use `scikit-learn`'s `CountVectorizer`. `CountVectorizer` replaces each tweet with a vector (think a list) of counts. Each position in the vector represents a unique word in the corpus. The value of an entry in a vector represents the number of times that word appeared in that tweet. Below, we restrict the length of the vectors to be 5,000 and the counts to be 0 (not in the tweet) and 1 (in the tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer(max_features=5000, binary=True)\n",
    "features = countvectorizer.fit_transform(table.select('clean_text').values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = table.select('airline_sentiment').values.reshape(-1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train/test datasets\n",
    "\n",
    "We don't want to train our classifier on the same dataset that we test it on, so let's split it into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, response, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification  <a id='classification'></a>\n",
    "\n",
    "OK, so now that we've turned our data into numbers, we're ready to feed it into a classifier. We're not going to concentrate too much on the code below, but here's the big picture. In the `fit_model` function defined below, we're going to use logistic regression as a classifier to take in the numerical representation of the tweets and spit out whether it's positive, neutral or negative. Then we'll use `test_model` to test the model's performance against our test data and print out some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def conmat(model, X_test, y_test):\n",
    "    \"\"\"Wrapper for sklearn's confusion matrix.\"\"\"\n",
    "    labels = model.classes_\n",
    "    y_pred = model.predict(X_test)\n",
    "    c = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(c, annot=True, fmt='d', \n",
    "                xticklabels=labels, \n",
    "                yticklabels=labels, \n",
    "                cmap=\"YlGnBu\", cbar=False)\n",
    "    plt.ylabel('Ground truth')\n",
    "    plt.xlabel('Prediction')\n",
    "    \n",
    "def test_model(model, X_test, y_test):\n",
    "    conmat(model, X_test, y_test)\n",
    "    print('Accuracy: ', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = fit_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use the `fit_random_forest` function below to train a random forest classifier on the training set and test the model on the test set. Which performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(X_train, y_train):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "rf = fit_random_forest(X_train, y_train)\n",
    "test_model(rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use the `test_tweet` function below to test your classifier's performance on a list of tweets. Write your tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tweets(tweets, model):\n",
    "    tweets = [clean_tweet(tweet) for tweet in tweets]\n",
    "    features = countvectorizer.transform(tweets)\n",
    "    predictions = model.predict(features)\n",
    "    return list(zip(tweets, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "my_tweets = [\"lol @justinbeiber and @BillGates are like soo #yesterday #amiright saw it on https://twitter.com #yolo\",\n",
    "            'omg I am never flying on Delta again',\n",
    "            'I love @VirginAmerica so much #friendlystaff']\n",
    "\n",
    "test_tweets(my_tweets, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret <a id='interpret'></a>\n",
    "\n",
    "Now we can interpret the classifier by the features that it found important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [(v,k) for k,v in countvectorizer.vocabulary_.items()]\n",
    "vocab = sorted(vocab, key=lambda x: x[0])\n",
    "vocab = [word for num,word in vocab]\n",
    "coef = list(zip(vocab, lr.coef_[0]))\n",
    "important = pd.DataFrame(lr.coef_).T\n",
    "important.columns = lr.classes_\n",
    "important['word'] = vocab\n",
    "important = Table.from_df(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important.sort('negative', descending=True).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important.sort('positive', descending=True).show(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
