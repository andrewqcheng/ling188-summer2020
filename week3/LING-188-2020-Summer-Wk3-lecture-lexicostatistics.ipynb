{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicostatistics\n",
    "In this lecture, we will learn about how to apply what you've learned about iteration and string manipulation to the study of cross-linguistic word analysis, or lexicostatistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell; don't worry about what it does yet.\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicostatistics is a method used in linguistics to determine the similarities between different languages by comparing words with common meanings. For example, cognate words are a common topic of study: the word for \"door\" in many Indo-European languages is similar: *thura* (Ancient Greek), *dvar* (Sanskrit), *dorus* (Celtic), *durn* (Armenian); but compare *porte* (French), *puerta* (Spanish), and *porta* (Italian).\n",
    "\n",
    "There are many ways to compare words cross-linguistically (across languages). One method is called the **Levenshtein distance** (or **edit distance**). The edit distance is the number of \"edits\" necessary to change one word into another word. An \"edit\" is the insertion, deletion, or replacement of a letter.\n",
    "\n",
    "To compute edit distance, I've provided a custom function called `edit_distance()`. The function has two arguments, `w1` and `w2` (word 1 and word 2). The output is the edit distance between `w1` and `w2` as an integer (whole number).\n",
    "\n",
    "Some examples:\n",
    "```\n",
    ">>> edit_distance('dog', 'doggy')\n",
    "2.0\n",
    ">>> edit_distance('dog', 'dag')\n",
    "1.0\n",
    ">>> edit_distance('dog', 'do')\n",
    "1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(w1, w2):\n",
    "    '''Computes the Levenshtein distance between two words.'''\n",
    "    size_x = len(w1) + 1\n",
    "    size_y = len(w2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if w1[x-1] == w2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    # print (matrix) (for debugging purposes)\n",
    "    return int(matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see how the `edit_distance` function works. How many \"edits\" do you need to go from \"dog\" to \"doge\"? How about \"cat\" to \"scallop\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edit_distance('dog', 'doge'))\n",
    "print(edit_distance('cat', 'scallop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Indo-European lexicostatistics database\n",
    "The data we are using is from [Dyen, Kruskal & Black (1992)](https://www.jstor.org/stable/1006517?seq=6#metadata_info_tab_contents). They collected a list of 200 cross-linguistically common words from over 80 Indo-European languages and dialects. The list of terms was originally compiled by [Swadesh (1952)](https://en.wikipedia.org/wiki/Swadesh_list) and is often called a \"Swadesh List\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Table.read_table('wk3-ie.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the table and get a feel for what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what other languages besides Afghan are in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.unique(l.column('Language'))\n",
    "print(u)\n",
    "print(\"There are\",len(u),\"languages in the Indo-European Lexicostatistics database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.where('Language','English ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will come in handy as a way to remember the words associated with each of the 200 values in the column `Feature`. (In Wednesday's lecture, Geoff will show you how to create a `dictionary` object that allows you to quickly look up any of the features.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The researchers who created the dataset, unfortunately, were unable to find words for all of the features for every single language. These are incorrectly loaded by the table as \"nan\". This stands for \"not a number\". Many programming languages have a value like this to fill in for missing data. Another common name is \"null\" or \"none\". Python's equivalent data type is the built-in `None` object. How many values are missing from the dataset? How many languages have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.where('Term','nan').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_wd = l.where('Term','nan').num_rows\n",
    "missing_lg = len(np.unique(l.where('Term','nan').column('Language')))\n",
    "print(\"The Indo-European Lexicostatistics Database is missing\",missing_wd,\"values in 'Term'.\")\n",
    "print(missing_lg,\"languages in the Indo-European Lexicostatistics Database are missing at least one value in 'Term'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace these \"nan\" values with an empty string (i.e., a string of length zero: \"\"). We could use a `for` loop or the method `.apply()` with a custom function. I will demonstrate both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = make_array()\n",
    "for item in l.column('Term'):\n",
    "    r = item.replace('nan','')\n",
    "    t1 = np.append(t1,r)\n",
    "l0 = l.with_columns('Term2', t1)\n",
    "l0.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.with_columns` can be used to create a new column (`Term2`) or replace the original one (`Term`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_replace = l.with_columns('Term', t1)\n",
    "l_replace.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_wd = l0.where('Term2','nan').num_rows\n",
    "whitespace = l0.where('Term2','').num_rows\n",
    "print(\"The Indo-European Lexicostatistics Database is missing\",missing_wd,\"values in 'Term2'.\")\n",
    "print(\"However, it has\",whitespace,\"values in 'Term2' that are empty strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that the `for` loop is pretty slow when you have a Table with thousands of rows. In this case, using `.apply()` is much faster in terms of processing speed. First, we create a custom function for the first argument of `.apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(s):\n",
    "    '''[Replace this with a description of what this function does.]'''\n",
    "    return s.replace('nan','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = l.apply(remove_nan,'Term')\n",
    "l0 = l.with_columns('Term2',t2) # This will overwrite our previous object l0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_wd = l0.where('Term2','nan').num_rows\n",
    "whitespace = l0.where('Term2','').num_rows\n",
    "print(\"The Indo-European Lexicostatistics Database is missing\",missing_wd,\"values in 'Term2'.\")\n",
    "print(\"However, it has\",whitespace,\"values in 'Term2' that are empty strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features with multiple terms\n",
    "Some languages use multiple words to refer to the same feature (e.g., synonyms). In this dataset, these are separated by commas. We are going to create a table called `multiple_terms` with two columns:\n",
    "\n",
    "- language: the name of the language\n",
    "- num_multiple: the number of features which contain multiple values \n",
    "\n",
    "We compute \"num_multiple\" by counting the number of features which contains commas for each language. `.apply()` and a `for` loop will come in handy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_comma(s):\n",
    "    '''[Replace this with a description of what this function does.]'''\n",
    "    return s.count(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it for one language first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'Afghan'\n",
    "group = l0.where('Language',language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_count = group.apply(count_comma,'Term2')\n",
    "comma_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a simple `Boolean` (or a \"True/False\" statement) to find out which items in the array had a comma. Any value in `comma_count` greater than 0 would indicate as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can count up the number of `True` items in the array. (What's wrong with just using `.sum()` on `comma_count`?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_commas = (comma_count > 0).sum()\n",
    "sum_commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give us an incorrect answer.\n",
    "comma_count.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put these lines of code within a `for` loop so that we can iterate over every language. Each time we iterate, we add another item to the array `num_multiple`, which we've created outside of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_multiple = make_array()\n",
    "for language in np.unique(l0.column('Language')): # be careful not to iterate over every single row in the Table!\n",
    "    group = l0.where('Language',language)\n",
    "    comma_count = group.apply(count_comma,'Term2')\n",
    "    sum_commas = (comma_count > 0).sum()\n",
    "    num_multiple = np.append(num_multiple, sum_commas)\n",
    "num_multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `Table().with_columns()` to turn this array into a `Table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_terms = Table().with_columns('language',np.unique(l0.column('Language')),'num_multiple',num_multiple)\n",
    "multiple_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,15)) # This will make our plot better proportioned\n",
    "plt.barh('language','num_multiple',data=multiple_terms)\n",
    "plt.title('Number of features with multiple terms per language')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Number of multiple terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Provencal has a lot of synonyms! Does this end up making the lexicon of Provencal a lot larger than English? To find out, let's create a second table called `unique_terms` which contains two columns:\n",
    "\n",
    "- language: the name of the language\n",
    "- num_terms: the number of unique terms\n",
    "\n",
    "And we'll create a different custom function that deals with a value that has multiple terms, separated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comma_split(s):\n",
    "    '''[Replace this with a description of what this function does.]'''\n",
    "    return s.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = l0.apply(comma_split,'Term2')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `comma_split()` function has returned an array of lists instead of an array of arrays. An array that contains `[a, b, [c, d], e]` will be considered to have 4 items, not 5. In order to accurately count up the items in the array, we need to \"flatten\" these lists and end up with something like `[a, b, c, d, e]`. To do this, we can use more `for` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notflat = make_array('a', 'b', ['c', 'd'], 'e')\n",
    "flat_array = make_array()\n",
    "for sublist in notflat:\n",
    "    for item in sublist:\n",
    "        flat_array = np.append(flat_array,item)\n",
    "flat_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can now use `comma_split` and the flattening `for` loop on `l0`. But we don't want to apply the function to the entire table `l0` outright; instead, we are going to use a series of `for` loops to do one language at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_terms = make_array()\n",
    "for language in np.unique(l0.column('Language')):\n",
    "    group = l0.where('Language',language)\n",
    "    terms = group.apply(comma_split,'Term2')\n",
    "    flat_array = make_array()\n",
    "    for sublist in terms:\n",
    "        for item in sublist:\n",
    "            flat_array = np.append(flat_array,item)\n",
    "    count = len(np.unique(flat_array))\n",
    "    num_terms = np.append(num_terms, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_terms = Table().with_columns('language',np.unique(l0.column('Language')),'num_terms', num_terms)\n",
    "unique_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,20)) # This will make our plot better proportioned\n",
    "ax = plt.subplot(1,2,1) # 1 row, 2 columns, plot #1\n",
    "plt.barh('language','num_multiple',data=multiple_terms)\n",
    "plt.title('Number of features with multiple terms per language')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Number of multiple terms')\n",
    "\n",
    "ax = plt.subplot(1,2,2) # 1 row, 2 columns, plot #2\n",
    "plt.barh('language','num_terms',data=unique_terms)\n",
    "plt.title('Number of unique terms per language')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Number of unique terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a different perspective, let's create the same plots, but sort the languages in decreasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,20)) # This will make our plot better proportioned\n",
    "ax = plt.subplot(1,2,1) # 1 row, 2 columns, plot #1\n",
    "plt.barh('language','num_multiple',data=multiple_terms.sort('num_multiple'))\n",
    "plt.title('Number of features with multiple terms per language')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Number of multiple terms')\n",
    "\n",
    "ax = plt.subplot(1,2,2) # 1 row, 2 columns, plot #2\n",
    "plt.barh('language','num_terms',data=unique_terms.sort('num_terms'))\n",
    "plt.title('Number of unique terms per language')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Number of unique terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which plot, left or right, do you think is a better visualization of the differences between lexicons? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the language distance\n",
    "Now, back to our main task. We want to compute language distance using the function `edit_distance`. To start out, we will compute the language distance between English and Afghan (labeled \"English ST\" and \"Afghan\" in this dataset). We are going to say that the language distance is the sum of the edit distances for all of the features (so `language_dist = dist_feature1 + dist_feature2 ... dist_feature200`). First, we will create an array with the distances for each feature. Then, we will compute the language distance using this array.\n",
    "\n",
    "- If a feature has multiple terms, take the first term.\n",
    "- Treat terms with spaces as if they were a single word (e.g., treat \"ta nezde\" as \"tanezde\").\n",
    "- Ignore missing values: these will create a small amount of error in our data, which is okay to ignore for now.\n",
    "- Use `for` statements to loop through the features.\n",
    "\n",
    "We will use two string methods we've just learned: `.split(',')` and `.replace(old, new)`. These are built-in methods which you can use with any string:\n",
    "```\n",
    ">>> 'cat,dog'.split(',')\n",
    "['cat, 'dog']\n",
    ">>> 'cat,dog'.replace('cat', 'fish)'\n",
    "'fish,dog'\n",
    "```\n",
    "And we will include them in our own custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten(s):\n",
    "    '''[Replace this with a description of what this function does.]'''\n",
    "    return s.replace(\" \",\"\").split(\",\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Term3 = make_array()\n",
    "for term in l0.column('Term2'):\n",
    "    t3 = shorten(term)\n",
    "    Term3 = np.append(Term3,t3)\n",
    "l1 = l0.with_columns('Term3',Term3)\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = l1.where('Language','English ST').column('Term3')\n",
    "a = l1.where('Language','Afghan').column('Term3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distance(e[0],a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = make_array()\n",
    "for i in np.arange(200):\n",
    "    distance = edit_distance(e[i],a[i])\n",
    "    array = np.append(array,distance)\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's put our lines of code inside of a `for` loop to compare \"English ST\" to every other language in `l1` (including itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = make_array()\n",
    "for language in np.unique(l1.column('Language')):\n",
    "    lang = l1.where('Language',language).column('Term3')\n",
    "    engl = l1.where('Language','English ST').column('Term3')\n",
    "    array = make_array()\n",
    "    for i in np.arange(200):\n",
    "        distance = edit_distance(lang[i],engl[i])\n",
    "        array = np.append(array,distance)\n",
    "    ld = np.sum(array)\n",
    "    matrix = np.append(matrix,ld)\n",
    "    print(\"Finished appending\",language)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `matrix` object is an array that we will use to create our final `Table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_table = Table().with_columns('Language',np.unique(l.column('Language')),'Language Distance',matrix)\n",
    "matrix_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to your computed edit distance, which language appears to be the most closely related to English? The most distantly related? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_table.sort('Language Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of a way to make `matrix_table` into an actual matrix, with additional columns for all 87 languages in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "A core aspect of datascience involves visualizations. We currently don't have the tools to make a good visualization with this matrix. Even if you cannot create the figure right now, what do you think would be a good way to plot the language distances? If you can think of a way to showcase the langauge distances using what we have learned so far, do so below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
